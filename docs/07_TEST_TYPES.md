# テスト形式の分類と UI設計

## 現在の有効設定

> **Testing Brain では以下 5形式 を使用中です。**
> `config.json` の `enabled_types` で変更可能です。
>
> ✅ **Unit** · **E2E** · **Visual** · **Snapshot** · **i18n**
>
> 下記の全14形式のドキュメントは、初心者がどのようなテスト形式が存在するかを学ぶためのリファレンスとして残しています。

---

## 前提

AIエージェントが **テストフォルダ配下を管理** し、以下を **データ層（JSON）に書き込む**:
- カバレッジ
- 観点
- 作成率（テストコード作成の進捗）
- 消化率（テスト実行の進捗）

GUIは **事前に決めた形式ごとのUIテンプレート** にデータを当てはめて表示する。

---

## WEBアプリにおけるテスト形式の分類

テストピラミッドに沿って、**実際にファイルが存在しうる形式** を整理する。

```
             ╱╲
            ╱  ╲        E2E（少数・高コスト・遅い）
           ╱────╲
          ╱      ╲      Integration（中程度）
         ╱────────╲
        ╱          ╲    Unit（多数・低コスト・速い）
       ╱────────────╲
```

### レイヤー1: Unit テスト

| 項目 | 内容 |
|------|------|
| **対象** | 関数、メソッド、クラス単体 |
| **ファイル命名例** | `*.test.ts`, `*.spec.ts`, `__tests__/*.ts` |
| **ツール例** | Jest, Vitest, Mocha |
| **AIが書くデータ** | テスト対象の関数名、カバレッジ率（行/分岐/関数）、pass/fail結果 |
| **特徴** | 最も多い、最も速い、最もコストが低い |

#### UI設計案
- **ファイルツリー形式**: ソースコード構造に対応するツリーで表示
- **色分け**: カバレッジ率に応じた緑→黄→赤のヒートマップ
- **指標**: 行カバレッジ%, 分岐カバレッジ%, 関数カバレッジ%

---

### レイヤー2: Integration テスト

| 項目 | 内容 |
|------|------|
| **対象** | コンポーネント間の結合、API↔DB接続、フック↔コンポーネント |
| **ファイル命名例** | `*.integration.test.ts`, `*.spec.ts`（同上の場合も） |
| **ツール例** | Jest + Testing Library, Supertest, MSW |
| **AIが書くデータ** | テスト対象のモジュール組み合わせ、結合パターン、pass/fail結果 |
| **特徴** | モジュール間のインタラクションを検証 |

#### UI設計案
- **依存関係グラフ**: モジュール間の矢印付き図
- **結合マトリクス**: 「A↔Bはテスト済み、A↔Cは未テスト」の表形式
- **指標**: 結合パターンカバレッジ%, テスト済みパス数

---

### レイヤー3: E2E テスト

| 項目 | 内容 |
|------|------|
| **対象** | ユーザーシナリオ全体（ブラウザ操作） |
| **ファイル命名例** | `e2e/*.spec.ts`, `cypress/e2e/*.cy.ts`, `tests/*.spec.ts` |
| **ツール例** | Playwright, Cypress, Selenium |
| **AIが書くデータ** | シナリオ名、ステップ数、pass/fail結果、所要時間 |
| **特徴** | 最も少ない、最も遅い、最もコストが高い |

#### UI設計案
- **シナリオフロー図**: ユーザー操作の流れを可視化
- **ステップ進捗**: 各ステップのpass/fail状態をタイムライン表示
- **指標**: シナリオカバレッジ%, 合格率%, 平均実行時間

---

### レイヤー4: API テスト

| 項目 | 内容 |
|------|------|
| **対象** | REST/GraphQL エンドポイント |
| **ファイル命名例** | `api/*.test.ts`, `*.api.spec.ts` |
| **ツール例** | Supertest, Pactum, REST Client |
| **AIが書くデータ** | エンドポイント一覧、メソッド、ステータスコード別テスト有無、レスポンス検証 |
| **特徴** | バックエンドの公開インタフェース |

#### UI設計案
- **エンドポイント一覧表**: `GET /api/users` → テスト有無 ✅❌
- **ステータスコード別**: 200, 400, 401, 404, 500 それぞれにテストがあるか
- **指標**: エンドポイントカバレッジ%, レスポンスコードカバレッジ%

---

### レイヤー5: Visual Regression テスト

| 項目 | 内容 |
|------|------|
| **対象** | UIの見た目の変更検出 |
| **ファイル命名例** | `__snapshots__/*.snap`, `*.stories.ts`（Storybook） |
| **ツール例** | Storybook + Chromatic, Percy, BackstopJS |
| **AIが書くデータ** | コンポーネント名、スナップショット有無、最終更新日、差分検出結果 |
| **特徴** | 見た目の一貫性を自動検証 |

#### UI設計案
- **ビフォー/アフター比較**: スクリーンショットの差分表示
- **コンポーネントギャラリー**: 各コンポーネントのスナップ状態一覧
- **指標**: スナップショットカバレッジ%, 差分検出数

---

### レイヤー6: Accessibility テスト

| 項目 | 内容 |
|------|------|
| **対象** | WCAG準拠、スクリーンリーダー対応 |
| **ファイル命名例** | `*.a11y.test.ts`, axe設定ファイル |
| **ツール例** | axe-core, Pa11y, Lighthouse |
| **AIが書くデータ** | ページ別のWCAG違反数、重大度、修正状態 |
| **特徴** | 法的要件にも関わる |

#### UI設計案
- **ページ別スコアカード**: 各ページのa11yスコア（0-100）
- **違反リスト**: 重大度別（Critical, Serious, Minor）の一覧
- **指標**: WCAG準拠率%, 未修正の違反数

---

### レイヤー7: Performance テスト

| 項目 | 内容 |
|------|------|
| **対象** | ページロード、API応答、メモリ、バンドルサイズ |
| **ファイル命名例** | `*.perf.test.ts`, Lighthouse設定 |
| **ツール例** | Lighthouse, k6, Web Vitals |
| **AIが書くデータ** | Core Web Vitals（LCP, FID, CLS）、バンドルサイズ、API応答時間 |
| **特徴** | 閾値ベースのpass/fail判定 |

#### UI設計案
- **スコアダッシュボード**: Core Web Vitals のゲージ表示
- **トレンドグラフ**: 時系列でのパフォーマンス推移
- **指標**: Lighthouseスコア, バンドルサイズ(KB), 応答時間(ms)

---

### レイヤー8: Security テスト

| 項目 | 内容 |
|------|------|
| **対象** | XSS, CSRF, SQLi, 認証・認可 |
| **ファイル命名例** | `*.security.test.ts`, OWASP設定 |
| **ツール例** | OWASP ZAP, Snyk, npm audit |
| **AIが書くデータ** | 脆弱性スキャン結果、依存関係の脆弱性、OWASP Top 10カバレッジ |
| **特徴** | リスクベースの優先順位付けが重要 |

#### UI設計案
- **リスクマトリクス**: 影響度×発生確率のグリッド
- **OWASP Top 10チェックリスト**: 各項目のカバー状態
- **指標**: 脆弱性数（Critical/High/Medium/Low）, 修正率%

---

## 共通の進捗指標（全テスト形式に適用）

| 指標 | 定義 | 計算式 |
|------|------|--------|
| **作成率** | テストコードがどれだけ書かれたか | 作成済みテストファイル数 / 必要なテストファイル数 |
| **消化率** | 書いたテストがどれだけ実行されたか | 実行済みテスト数 / 作成済みテスト数 |
| **合格率** | 実行したテストがどれだけ通ったか | pass数 / 実行済みテスト数 |
| **カバレッジ** | ソースコードのどれだけがカバーされているか | テスト対象行数 / 全行数 |
| **観点網羅率** | 検討すべき観点がどれだけカバーされているか | テスト有り観点 / 全観点 |

---

## まとめ：形式ごとのUI一覧

| テスト形式 | 主なUI表現 | キーメトリクス |
|-----------|-----------|--------------|
| Unit | ファイルツリー + ヒートマップ | 行カバレッジ%, 関数カバレッジ% |
| Integration | 依存関係グラフ + 結合マトリクス | 結合パターンカバレッジ% |
| E2E | シナリオフロー図 + タイムライン | シナリオカバレッジ%, 合格率% |
| API | エンドポイント一覧表 | エンドポイントカバレッジ% |
| Visual | コンポーネントギャラリー + 差分表示 | スナップショットカバレッジ% |
| Accessibility | ページ別スコアカード + 違反リスト | WCAG準拠率% |
| Performance | スコアゲージ + トレンドグラフ | Lighthouseスコア |
| Security | リスクマトリクス + チェックリスト | 脆弱性数, 修正率% |

---

## 次の判断ポイント

1. **どの形式を最初にサポートするか？**（全部同時は大変）
2. **形式ごとにデータ層のJSON構造が異なる** — それぞれ個別に設計する必要あり
3. **GUIのUIテンプレート** — 形式ごとに画面コンポーネントを作る必要あり
4. **AIスキル** — 形式ごとにAIの操作手順が異なる

> 優先度の提案:
> **Unit → E2E → API → Integration → 残り** の順が実用的では？
